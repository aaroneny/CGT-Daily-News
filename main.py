import feedparser
import datetime
import pytz
from deep_translator import GoogleTranslator
from time import mktime
import re

# ================= é…ç½®åŒº =================

# --- 1. å…¨çƒæº (Global / FDA / In vivo) ---
GLOBAL_RSS_URLS = [
    # In vivo CAR-T & åŸºå› ç¼–è¾‘ (é«˜ä¼˜å…ˆçº§)
    "https://news.google.com/rss/search?q=(site:businesswire.com+OR+site:prnewswire.com)+AND+%22In+vivo%22+AND+(%22CAR-T%22+OR+%22Gene+Editing%22)+when:1d&hl=en-US&gl=US&ceid=US:en",
    # å¸¸è§„ FDA / CAR-T è¿›å±•
    "https://news.google.com/rss/search?q=(site:businesswire.com+OR+site:prnewswire.com+OR+site:fda.gov)+AND+(CAR-T+OR+%22Cell+Therapy%22)+AND+(IND+OR+FDA+OR+Approval+OR+Clinical)+when:1d&hl=en-US&gl=US&ceid=US:en"
]

# --- 2. ä¸­å›½æº (China / NMPA / CDE) ---
# æœç´¢é€»è¾‘ï¼šæœç´¢ä¸­æ–‡å…³é”®è¯ (NMPA, CDE, è·æ‰¹, IND) æˆ– è‹±æ–‡å…³äºä¸­å›½çš„æŠ¥é“
CHINA_RSS_URLS = [
    # ä¸­æ–‡æœç´¢ï¼šç»†èƒ/åŸºå› æ²»ç–— + ç›‘ç®¡/è¿›å±•
    "https://news.google.com/rss/search?q=(%E7%BB%86%E8%83%9E%E6%B2%BB%E7%96%97+OR+CAR-T+OR+%E5%9F%BA%E5%9B%A0%E6%B2%BB%E7%96%97)+AND+(NMPA+OR+CDE+OR+%E8%8E%B7%E6%89%B9+OR+%E4%B8%B4%E5%BA%8A+OR+IND)+when:1d&hl=zh-CN&gl=CN&ceid=CN:zh-Hans",
    # è‹±æ–‡æœç´¢ï¼šChina + Biotech å…³é”®è¯
    "https://news.google.com/rss/search?q=China+AND+(CAR-T+OR+%22Cell+Therapy%22)+AND+(NMPA+OR+IND+OR+Approval)+when:1d&hl=en-US&gl=US&ceid=US:en"
]

# å…¨çƒå…³é”®è¯ç™½åå•
GLOBAL_KEYWORDS = [
    "In vivo", "CAR-T", "TCR-T", "NK", "FDA", "IND", "Approval", "Clinical", "Pipeline", "LNP"
]

# ä¸­å›½å…³é”®è¯ç™½åå• (åŒ…å«ä¸­æ–‡)
CHINA_KEYWORDS = [
    "NMPA", "CDE", "IND", "å—ç†", "è·æ‰¹", "ä¸´åºŠ", "è¯•éªŒ", "ç”³è¯·", "è¯ç›‘å±€", 
    "China", "Chinese", "Approval", "Cleared", "CAR-T", "Cell Therapy"
]

# æ’é™¤å™ªéŸ³
EXCLUDE_WORDS = ["Market size", "Report", "Forecast", "Stock", "Dividend", "å¸‚åœºè§„æ¨¡", "ç ”æŠ¥", "è‚¡ä»·", "é¢„æµ‹"]

# ================= æ ¸å¿ƒé€»è¾‘ =================

def is_recent(published_parsed):
    """24å°æ—¶ç†”æ–­æœºåˆ¶"""
    if not published_parsed: return False
    news_time = datetime.datetime.fromtimestamp(mktime(published_parsed)).replace(tzinfo=pytz.utc)
    current_time = datetime.datetime.now(pytz.utc)
    return (current_time - news_time).total_seconds() <= 86400

def highlight_title(title):
    """è§†è§‰æ ‡è®°"""
    flags = []
    if re.search(r"In\s*vivo", title, re.IGNORECASE):
        flags.append("ğŸ”¥In-vivo")
    if re.search(r"FDA|NMPA|Approval|è·æ‰¹|Approved", title, re.IGNORECASE):
        flags.append("ğŸ›ï¸ç›‘ç®¡")
    
    if flags:
        return f"{' '.join(flags)} | {title}"
    return title

def fetch_group_news(urls, keywords, group_name):
    """é€šç”¨æŠ“å–å‡½æ•°ï¼šæ ¹æ®ä¼ å…¥çš„ URL å’Œ å…³é”®è¯ è·å–æ–°é—»"""
    news_items = []
    seen_links = set()
    translator = GoogleTranslator(source='auto', target='zh-CN')

    print(f"æ­£åœ¨æ‰«æ {group_name} æ•°æ®æº...")

    for url in urls:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            title = entry.title
            link = entry.link
            
            if not is_recent(entry.published_parsed): continue
            if link in seen_links: continue
            seen_links.add(link)

            # å…³é”®è¯è¿‡æ»¤
            if any(k.lower() in title.lower() for k in keywords) and \
               not any(e.lower() in title.lower() for e in EXCLUDE_WORDS):
                
                clean_title = title.rsplit(' - ', 1)[0]
                
                # å¦‚æœæ˜¯è‹±æ–‡ï¼Œç¿»è¯‘æˆä¸­æ–‡ï¼›å¦‚æœæ˜¯ä¸­æ–‡ï¼Œç›´æ¥ç”¨
                # ç®€å•åˆ¤æ–­ï¼šå¦‚æœæ ‡é¢˜åŒ…å«ä¸­æ–‡å¸¸è§æ ‡ç‚¹æˆ–æ±‰å­—æ¯”ä¾‹é«˜ï¼Œåˆ™ä¸ç¿»è¯‘
                is_chinese_text = bool(re.search(r'[\u4e00-\u9fa5]', clean_title))
                
                if not is_chinese_text:
                    try:
                        title_disp = translator.translate(clean_title)
                    except:
                        title_disp = clean_title
                else:
                    title_disp = clean_title # åŸç”Ÿä¸­æ–‡ä¸ç¿»è¯‘

                news_dt = datetime.datetime.fromtimestamp(mktime(entry.published_parsed)).replace(tzinfo=pytz.utc)
                beijing_dt = news_dt.astimezone(pytz.timezone('Asia/Shanghai'))
                
                news_items.append({
                    "title_show": highlight_title(title_disp),
                    "title_origin": clean_title,
                    "link": link,
                    "date_str": beijing_dt.strftime('%H:%M'),
                    "timestamp": news_dt.timestamp()
                })
    
    news_items.sort(key=lambda x: x["timestamp"], reverse=True)
    return news_items

def generate_markdown(global_news, china_news):
    beijing_tz = pytz.timezone('Asia/Shanghai')
    now = datetime.datetime.now(beijing_tz)
    today_str = now.strftime("%Y-%m-%d")
    update_time_str = now.strftime("%H:%M")

    # 1. é™æ€å¤´éƒ¨ (æ¯æ¬¡é‡å†™)
    md_content = f"""# ğŸ§¬ CGT æ¯æ—¥æƒ…æŠ¥ (Daily Brief)
> **æ—¥æœŸ**: {today_str} | **æ›´æ–°æ—¶é—´**: {update_time_str} (åŒ—äº¬æ—¶é—´)
> **ç›‘æ§èŒƒå›´**: Global (In vivo/FDA) & China (NMPA/Biotech)

---

"""

    # 2. ç”Ÿæˆ Global æ¿å—
    md_content += "## ğŸŒ å…¨çƒå‰æ²¿ (FDA / In vivo / MNCs)\n"
    if global_news:
        for item in global_news:
            md_content += f"- `[{item['date_str']}]` **{item['title_show']}**\n  <br><small>ğŸ‡¬ğŸ‡§ *{item['title_origin']}* [ğŸ”—Source]({item['link']})</small>\n"
    else:
        md_content += "- *å½“å‰æš‚æ— è¿‡å» 24 å°æ—¶å†…çš„ç›¸å…³é‡ç£…å…¨çƒèµ„è®¯ã€‚*\n"
    
    md_content += "\n---\n\n"

    # 3. ç”Ÿæˆ China æ¿å—
    md_content += "## ğŸ‡¨ğŸ‡³ ä¸­å›½åŠ¨æ€ (NMPA / Domestic Players)\n"
    if china_news:
        for item in china_news:
            # å¦‚æœåŸæ–‡å°±æ˜¯ä¸­æ–‡ï¼Œå°±ä¸æ˜¾ç¤ºç¬¬äºŒè¡Œè‹±æ–‡åŸé¢˜äº†ï¼Œä¿æŒç®€æ´
            is_origin_cn = bool(re.search(r'[\u4e00-\u9fa5]', item['title_origin']))
            if is_origin_cn:
                md_content += f"- `[{item['date_str']}]` **{item['title_show']}** [ğŸ”—é˜…è¯»åŸæ–‡]({item['link']})\n"
            else:
                md_content += f"- `[{item['date_str']}]` **{item['title_show']}**\n  <br><small>ğŸ‡¬ğŸ‡§ *{item['title_origin']}* [ğŸ”—Source]({item['link']})</small>\n"
    else:
        md_content += "- *å½“å‰æš‚æ— è¿‡å» 24 å°æ—¶å†…çš„ç›¸å…³ä¸­å›½åŒºæœ€æ–°èµ„è®¯ã€‚*\n"

    return md_content

def update_readme(content):
    # è¦†ç›–å†™å…¥æ¨¡å¼ 'w'ï¼Œç¡®ä¿å½»åº•æ¸…é™¤æ—§å†…å®¹
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(content)

if __name__ == "__main__":
    # åˆ†åˆ«æŠ“å–
    global_items = fetch_group_news(GLOBAL_RSS_URLS, GLOBAL_KEYWORDS, "å…¨çƒç»„")
    china_items = fetch_group_news(CHINA_RSS_URLS, CHINA_KEYWORDS, "ä¸­å›½ç»„")
    
    # ç”Ÿæˆå¹¶å†™å…¥
    full_content = generate_markdown(global_items, china_items)
    update_readme(full_content)

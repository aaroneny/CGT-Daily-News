import feedparser
import datetime
import pytz
import re

# 1. é…ç½®æ•°æ®æº (Google News RSS é’ˆå¯¹ç‰¹å®šå…³é”®è¯)
# ä½ å¯ä»¥æ·»åŠ å¤šä¸ªå…³é”®è¯ç»„åˆ
RSS_URLS = [
    "https://news.google.com/rss/search?q=Cell+Gene+Therapy+FDA+IND&hl=en-US&gl=US&ceid=US:en",
    "https://news.google.com/rss/search?q=CAR-T+approval+pipeline&hl=en-US&gl=US&ceid=US:en",
    "https://news.google.com/rss/search?q=biotech+series+funding+cell+therapy&hl=en-US&gl=US&ceid=US:en"
]

# 2. å…³é”®è¯è¿‡æ»¤ (ç®€å•çš„è§„åˆ™å¼•æ“)
# åªæœ‰åŒ…å«è¿™äº›å…³é”®è¯çš„æ–°é—»æ‰ä¼šè¢«ä¿ç•™
KEYWORDS = ["FDA", "IND", "approval", "cleared", "clinical trial", "submission", "green light", "Series A", "Series B"]
EXCLUDE_WORDS = ["market report", "stocks", "forecast", "size", "share"] # æ’é™¤æ‰æ— ç”¨çš„å¸‚åœºåˆ†ææŠ¥å‘Š

def fetch_news():
    news_items = []
    seen_links = set() # å»é‡

    for url in RSS_URLS:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            title = entry.title
            link = entry.link
            pub_date = entry.published
            
            # ç®€å•å»é‡
            if link in seen_links:
                continue
            seen_links.add(link)

            # è¿‡æ»¤é€»è¾‘
            if any(k.lower() in title.lower() for k in KEYWORDS) and \
               not any(e.lower() in title.lower() for e in EXCLUDE_WORDS):
                news_items.append({
                    "title": title,
                    "link": link,
                    "date": pub_date
                })
    return news_items

def update_readme(news_items):
    beijing_tz = pytz.timezone('Asia/Shanghai')
    now = datetime.datetime.now(beijing_tz).strftime("%Y-%m-%d %H:%M:%S")
    
    with open("README.md", "r", encoding="utf-8") as f:
        content = f.read()

    # å¯»æ‰¾æ ‡è®°ä½ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ‰‹åŠ¨æ·»åŠ 
    header_marker = "## ğŸ§¬ æœ€æ–° CGT è¡Œä¸šåŠ¨æ€"
    if header_marker not in content:
        new_content = content + f"\n\n{header_marker}\n\næ›´æ–°æ—¶é—´: {now}\n\n"
    else:
        # æˆªæ–­æ—§å†…å®¹ï¼Œä¿ç•™ Header ä¹‹å‰çš„éƒ¨åˆ†
        new_content = content.split(header_marker)[0] + f"{header_marker}\n\næ›´æ–°æ—¶é—´: {now}\n\n"

    # ç”Ÿæˆ Markdown åˆ—è¡¨
    for item in news_items:
        # æ¸…ç† Google News æ ‡é¢˜ä¸­çš„æ¥æºåç¼€ (ä¾‹å¦‚ " - PR Newswire")
        clean_title = item['title'].rsplit(' - ', 1)[0]
        new_content += f"- **{clean_title}** ([é“¾æ¥]({item['link']}))\n"
    
    # å¦‚æœæ²¡æœ‰æ–°é—»
    if not news_items:
        new_content += "- ä»Šæ—¥æš‚æ— ç¬¦åˆæ¡ä»¶çš„é‡è¦èµ„è®¯ã€‚\n"

    with open("README.md", "w", encoding="utf-8") as f:
        f.write(new_content)

if __name__ == "__main__":
    items = fetch_news()
    update_readme(items)

import feedparser
import datetime
import pytz
from deep_translator import GoogleTranslator
from time import mktime
import re
import os       # æ–°å¢
import requests # æ–°å¢

# ================= é…ç½®åŒº =================

# --- 1. å…¨çƒæº (Global / FDA / In vivo) ---
GLOBAL_RSS_URLS = [
    "https://news.google.com/rss/search?q=(site:businesswire.com+OR+site:prnewswire.com)+AND+%22In+vivo%22+AND+(%22CAR-T%22+OR+%22Gene+Editing%22)+when:1d&hl=en-US&gl=US&ceid=US:en",
    "https://news.google.com/rss/search?q=(site:businesswire.com+OR+site:prnewswire.com+OR+site:fda.gov)+AND+(CAR-T+OR+%22Cell+Therapy%22)+AND+(IND+OR+FDA+OR+Approval+OR+Clinical)+when:1d&hl=en-US&gl=US&ceid=US:en"
]

# --- 2. ä¸­å›½æº (China / NMPA / CDE) ---
CHINA_RSS_URLS = [
    "https://news.google.com/rss/search?q=(%E7%BB%86%E8%83%9E%E6%B2%BB%E7%96%97+OR+CAR-T+OR+%E5%9F%BA%E5%9B%A0%E6%B2%BB%E7%96%97)+AND+(NMPA+OR+CDE+OR+%E8%8E%B7%E6%89%B9+OR+%E4%B8%B4%E5%BA%8A+OR+IND)+when:1d&hl=zh-CN&gl=CN&ceid=CN:zh-Hans",
    "https://news.google.com/rss/search?q=China+AND+(CAR-T+OR+%22Cell+Therapy%22)+AND+(NMPA+OR+IND+OR+Approval)+when:1d&hl=en-US&gl=US&ceid=US:en"
]

# å…³é”®è¯é…ç½®
GLOBAL_KEYWORDS = ["In vivo", "CAR-T", "TCR-T", "NK", "FDA", "IND", "Approval", "Clinical", "Pipeline", "LNP"]
CHINA_KEYWORDS = ["NMPA", "CDE", "IND", "å—ç†", "è·æ‰¹", "ä¸´åºŠ", "è¯•éªŒ", "ç”³è¯·", "è¯ç›‘å±€", "China", "Chinese", "Approval", "Cleared", "CAR-T", "Cell Therapy"]
EXCLUDE_WORDS = ["Market size", "Report", "Forecast", "Stock", "Dividend", "å¸‚åœºè§„æ¨¡", "ç ”æŠ¥", "è‚¡ä»·", "é¢„æµ‹"]

# ================= æ ¸å¿ƒé€»è¾‘ =================

def is_recent(published_parsed):
    """24å°æ—¶ç†”æ–­æœºåˆ¶"""
    if not published_parsed: return False
    news_time = datetime.datetime.fromtimestamp(mktime(published_parsed)).replace(tzinfo=pytz.utc)
    current_time = datetime.datetime.now(pytz.utc)
    return (current_time - news_time).total_seconds() <= 86400

def highlight_title(title):
    """è§†è§‰æ ‡è®°"""
    flags = []
    if re.search(r"In\s*vivo", title, re.IGNORECASE):
        flags.append("ğŸ”¥In-vivo")
    if re.search(r"FDA|NMPA|Approval|è·æ‰¹|Approved", title, re.IGNORECASE):
        flags.append("ğŸ›ï¸ç›‘ç®¡")
    
    if flags:
        return f"{' '.join(flags)} | {title}"
    return title

def fetch_group_news(urls, keywords, group_name):
    """é€šç”¨æŠ“å–å‡½æ•°"""
    news_items = []
    seen_links = set()
    translator = GoogleTranslator(source='auto', target='zh-CN')

    print(f"æ­£åœ¨æ‰«æ {group_name} æ•°æ®æº...")

    for url in urls:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            title = entry.title
            link = entry.link
            
            if not is_recent(entry.published_parsed): continue
            if link in seen_links: continue
            seen_links.add(link)

            if any(k.lower() in title.lower() for k in keywords) and \
               not any(e.lower() in title.lower() for e in EXCLUDE_WORDS):
                
                clean_title = title.rsplit(' - ', 1)[0]
                is_chinese_text = bool(re.search(r'[\u4e00-\u9fa5]', clean_title))
                
                if not is_chinese_text:
                    try:
                        title_disp = translator.translate(clean_title)
                    except:
                        title_disp = clean_title
                else:
                    title_disp = clean_title

                news_dt = datetime.datetime.fromtimestamp(mktime(entry.published_parsed)).replace(tzinfo=pytz.utc)
                beijing_dt = news_dt.astimezone(pytz.timezone('Asia/Shanghai'))
                
                news_items.append({
                    "title_show": highlight_title(title_disp),
                    "title_origin": clean_title,
                    "link": link,
                    "date_str": beijing_dt.strftime('%H:%M'),
                    "timestamp": news_dt.timestamp()
                })
    
    news_items.sort(key=lambda x: x["timestamp"], reverse=True)
    return news_items

def generate_markdown(global_news, china_news):
    beijing_tz = pytz.timezone('Asia/Shanghai')
    now = datetime.datetime.now(beijing_tz)
    today_str = now.strftime("%Y-%m-%d")
    update_time_str = now.strftime("%H:%M")

    md_content = f"""# ğŸ§¬ CGT æ¯æ—¥æƒ…æŠ¥ (Daily Brief)
> **æ—¥æœŸ**: {today_str} | **æ›´æ–°æ—¶é—´**: {update_time_str} (åŒ—äº¬æ—¶é—´)
> **ç›‘æ§èŒƒå›´**: Global (In vivo/FDA) & China (NMPA/Biotech)

---

"""
    md_content += "## ğŸŒ å…¨çƒå‰æ²¿ (FDA / In vivo / MNCs)\n"
    if global_news:
        for item in global_news:
            md_content += f"- `[{item['date_str']}]` **{item['title_show']}**\n  <br><small>ğŸ‡¬ğŸ‡§ *{item['title_origin']}* [ğŸ”—Source]({item['link']})</small>\n"
    else:
        md_content += "- *å½“å‰æš‚æ— è¿‡å» 24 å°æ—¶å†…çš„ç›¸å…³é‡ç£…å…¨çƒèµ„è®¯ã€‚*\n"
    
    md_content += "\n---\n\n"

    md_content += "## ğŸ‡¨ğŸ‡³ ä¸­å›½åŠ¨æ€ (NMPA / Domestic Players)\n"
    if china_news:
        for item in china_news:
            is_origin_cn = bool(re.search(r'[\u4e00-\u9fa5]', item['title_origin']))
            if is_origin_cn:
                md_content += f"- `[{item['date_str']}]` **{item['title_show']}** [ğŸ”—é˜…è¯»åŸæ–‡]({item['link']})\n"
            else:
                md_content += f"- `[{item['date_str']}]` **{item['title_show']}**\n  <br><small>ğŸ‡¬ğŸ‡§ *{item['title_origin']}* [ğŸ”—Source]({item['link']})</small>\n"
    else:
        md_content += "- *å½“å‰æš‚æ— è¿‡å» 24 å°æ—¶å†…çš„ç›¸å…³ä¸­å›½åŒºæœ€æ–°èµ„è®¯ã€‚*\n"

    return md_content

def update_readme(content):
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(content)

# ================= æ–°å¢ï¼šå¾®ä¿¡æ¨é€å‡½æ•° =================
def pushplus_notify(content):
    token = os.environ.get("PUSHPLUS_TOKEN")
    if not token:
        print("âš ï¸ æœªæ£€æµ‹åˆ° PUSHPLUS_TOKENï¼Œè·³è¿‡å¾®ä¿¡æ¨é€")
        return
    
    # è·å–å½“å‰æ—¥æœŸç”¨äºæ ‡é¢˜
    beijing_tz = pytz.timezone('Asia/Shanghai')
    today_str = datetime.datetime.now(beijing_tz).strftime("%Y-%m-%d")
    
    url = "http://www.pushplus.plus/send"
    payload = {
        "token": token,
        "title": f"ğŸ§¬ CGTæ—¥æŠ¥ | {today_str}",
        "content": content,
        "template": "markdown",
        "channel": "wechat"
    }
    try:
        resp = requests.post(url, json=payload)
        print(f"å¾®ä¿¡æ¨é€ç»“æœ: {resp.text}")
    except Exception as e:
        print(f"å¾®ä¿¡æ¨é€å¤±è´¥: {e}")

if __name__ == "__main__":
    # 1. æŠ“å–æ•°æ®
    global_items = fetch_group_news(GLOBAL_RSS_URLS, GLOBAL_KEYWORDS, "å…¨çƒç»„")
    china_items = fetch_group_news(CHINA_RSS_URLS, CHINA_KEYWORDS, "ä¸­å›½ç»„")
    
    # 2. ç”Ÿæˆå†…å®¹
    full_content = generate_markdown(global_items, china_items)
    
    # 3. æ›´æ–° README
    update_readme(full_content)
    
    # 4. å‘é€å¾®ä¿¡æ¨é€ (æ–°å¢æ­¥éª¤)
    print("æ­£åœ¨å‘é€å¾®ä¿¡æ¨é€...")
    pushplus_notify(full_content)

import feedparser
import datetime
import pytz
from deep_translator import GoogleTranslator
from time import mktime

# --- æ ¸å¿ƒé…ç½®åŒº ---

# 1. å‡çº§ç‰ˆ RSS æºï¼šé”å®š PR Newswire, Business Wire, GlobeNewswire (ä¸€æ‰‹ä¼ä¸šé€šç¨¿)
# ä½¿ç”¨ when:1d å¼ºåˆ¶åªæœ24å°æ—¶å†…ï¼Œå¹¶ç»„åˆä½ çš„æ ¸å¿ƒå…³é”®è¯
# è¯­æ³•è¯´æ˜ï¼š(æ¥æº1 OR æ¥æº2) AND (å…³é”®è¯ç»„åˆ)
RSS_URLS = [
    # ç»¼åˆæœç´¢ï¼šé™å®šåœ¨ä¸€æ‰‹é€šç¨¿å¹³å°ï¼Œæœç´¢ FDA, IND, CAR-T, In vivo ç­‰å…³é”®è¯
    "https://news.google.com/rss/search?q=(site:businesswire.com+OR+site:prnewswire.com+OR+site:globenewswire.com)+AND+(CAR-T+OR+%22Cell+Therapy%22+OR+%22Gene+Therapy%22+OR+%22In+vivo%22+OR+IND+OR+FDA)+when:1d&hl=en-US&gl=US&ceid=US:en",
    
    # è¡¥å……æœç´¢ï¼šé˜²æ­¢æ¼ç½‘ä¹‹é±¼ï¼Œé’ˆå¯¹ In vivo CAR-T çš„å…¨ç½‘æœ€æ–°ï¼ˆä¸ä»…ä»…æ˜¯é€šç¨¿ï¼‰
    "https://news.google.com/rss/search?q=%22In+vivo+CAR-T%22+when:1d&hl=en-US&gl=US&ceid=US:en"
]

# 2. å…³é”®è¯è¿‡æ»¤ï¼ˆç™½åå•ï¼‰- åªè¦æ ‡é¢˜åŒ…å«è¿™äº›è¯ä¸­çš„ä»»æ„ä¸€ä¸ªï¼Œå°±ä¿ç•™
KEYWORDS = [
    "FDA", "IND", "approval", "cleared", "clinical trial", "trial start", 
    "dosed", "fast track", "orphan drug", "submission", "pipeline", 
    "In vivo", "CAR-T", "TCR-T", "NK", "gene editing", "LNP", "delivery"
]

# 3. æ’é™¤è¯ï¼ˆé»‘åå•ï¼‰- è¿‡æ»¤æ‰éç ”å‘ç±»çš„å™ªéŸ³
EXCLUDE_WORDS = [
    "market size", "market report", "share", "forecast", "outlook", 
    "stock", "dividend", "loss", "profit", "quarterly result", "lawsuit"
]

def is_recent(published_parsed):
    """
    ä¸¥æ ¼æ£€æŸ¥æ–°é—»æ—¶é—´æ˜¯å¦åœ¨è¿‡å» 24 å°æ—¶å†…
    """
    if not published_parsed:
        return False
    
    # å°† RSS æ—¶é—´è½¬æ¢ä¸º UTC datetime
    news_time = datetime.datetime.fromtimestamp(mktime(published_parsed)).replace(tzinfo=pytz.utc)
    current_time = datetime.datetime.now(pytz.utc)
    
    # è®¡ç®—æ—¶é—´å·®
    diff = current_time - news_time
    
    # ä¹Ÿå°±æ˜¯ 24 å°æ—¶ (86400ç§’)
    if diff.total_seconds() <= 86400:
        return True
    return False

def fetch_news():
    news_items = []
    seen_links = set()
    translator = GoogleTranslator(source='auto', target='zh-CN')

    print("æ­£åœ¨æ‰«æå…¨çƒæœ€æ–°ä¸€æ‰‹é€šç¨¿ (Past 24h)...")

    for url in RSS_URLS:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            title = entry.title
            link = entry.link
            
            # 1. ä¸¥æ ¼çš„æ—¶é—´ç­›é€‰
            if not is_recent(entry.published_parsed):
                continue
            
            if link in seen_links:
                continue
            seen_links.add(link)

            # 2. å…³é”®è¯ç­›é€‰
            if any(k.lower() in title.lower() for k in KEYWORDS) and \
               not any(e.lower() in title.lower() for e in EXCLUDE_WORDS):
                
                # æ¸…ç†æ ‡é¢˜
                clean_title_en = title.rsplit(' - ', 1)[0]
                
                try:
                    title_zh = translator.translate(clean_title_en)
                except:
                    title_zh = "ç¿»è¯‘æš‚ä¸å¯ç”¨"

                # è®°å½•å‘å¸ƒæ—¶é—´ (è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´æ˜¾ç¤º)
                news_dt = datetime.datetime.fromtimestamp(mktime(entry.published_parsed)).replace(tzinfo=pytz.utc)
                beijing_dt = news_dt.astimezone(pytz.timezone('Asia/Shanghai'))
                date_str = beijing_dt.strftime('%m-%d %H:%M')

                news_items.append({
                    "title_zh": title_zh,
                    "title_en": clean_title_en,
                    "link": link,
                    "date_str": date_str,
                    "timestamp": news_dt.timestamp() # ç”¨äºåç»­æ’åº
                })
    
    # æŒ‰æ—¶é—´å€’åºæ’åˆ—ï¼ˆæœ€æ–°çš„åœ¨æœ€ä¸Šé¢ï¼‰
    news_items.sort(key=lambda x: x["timestamp"], reverse=True)
    return news_items

def update_readme(news_items):
    beijing_tz = pytz.timezone('Asia/Shanghai')
    today_str = datetime.datetime.now(beijing_tz).strftime("%Y-%m-%d")
    now_str = datetime.datetime.now(beijing_tz).strftime("%H:%M")
    
    with open("README.md", "r", encoding="utf-8") as f:
        content = f.read()

    # æˆ‘ä»¬æ¯å¤©åªç”Ÿæˆå½“å¤©çš„æ¿å—ï¼Œæˆ–è€…ç›´æ¥è¦†ç›–
    # è¿™é‡Œé‡‡ç”¨â€œç´¯åŠ æ¨¡å¼â€ï¼Œå¹¶åœ¨é¡¶éƒ¨æ˜¾ç¤ºâ€œä»Šæ—¥æœ€æ–°â€
    
    header_marker = "## ğŸš€ ä»Šæ—¥æœ€æ–° (Latest 24h)"
    
    # å¦‚æœè¦ä¿ç•™å†å²è®°å½•ï¼Œå¯ä»¥åœ¨è¿™é‡Œåšé€»è¾‘ï¼Œè¿™é‡Œä¸ºäº†ç®€æ´ï¼Œæˆ‘æ¼”ç¤ºâ€œæ¯æ¬¡æ›´æ–°è¦†ç›–æœ€æ–°åˆ—è¡¨â€
    # ä½†ä¿ç•™ä¸‹æ–¹çš„â€œå†å²å½’æ¡£â€ç»“æ„ï¼ˆå¦‚æœéœ€è¦å¯ä»¥æ•™ä½ æ€ä¹ˆåšå½’æ¡£ï¼‰
    # ç›®å‰é€»è¾‘ï¼šåˆ·æ–°æ•´ä¸ª README çš„æ–°é—»åŒºåŸŸ
    
    if header_marker not in content:
        # åˆå§‹åŒ–
        new_content_top = f"# ğŸ§¬ å…¨çƒ CGT æ¯æ—¥æƒ…æŠ¥\n\n{header_marker}\n> æ›´æ–°äºåŒ—äº¬æ—¶é—´: {today_str} {now_str}\n\n"
        old_content = "" # æˆ–è€…ä¿ç•™åŸæœ‰çš„ä»‹ç»
    else:
        new_content_top = content.split(header_marker)[0] + f"{header_marker}\n> æ›´æ–°äºåŒ—äº¬æ—¶é—´: {today_str} {now_str}\n\n"

    news_list = ""
    for item in news_items:
        # å¢åŠ æ—¶é—´æ ‡ç­¾
        news_list += f"- `[{item['date_str']}]` **{item['title_zh']}**<br><small>*{item['title_en']}* [ğŸ”—Source]({item['link']})</small>\n"
    
    if not news_items:
        news_list += "- æˆªè‡³ç›®å‰ï¼Œè¿‡å»24å°æ—¶å†…å…¨çƒä¸»è¦é€šç¨¿å¹³å°æš‚æ— ç›¸å…³é‡ç£…å‘å¸ƒã€‚\n"

    with open("README.md", "w", encoding="utf-8") as f:
        f.write(new_content_top + news_list)

if __name__ == "__main__":
    items = fetch_news()
    update_readme(items)
